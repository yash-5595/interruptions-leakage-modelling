{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93594a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# import optuna\n",
    "import mlflow\n",
    "import torch\n",
    "from mlflow import pytorch\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pprint import pformat\n",
    "from pathlib import Path\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import os\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s %(levelname)-8s %(message)s',\n",
    "    level=logging.INFO,\n",
    "    datefmt='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "from trainer import Trainer\n",
    "from encoder_decoder import EncoderDecoderWrapper\n",
    "\n",
    "import random\n",
    "import os\n",
    "from config import exp_dict_all\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pdp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0007d42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEGMENT = \"segment_1425_1430\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "24734f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 15:41:49 INFO     True, NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "params = Namespace(batch_size=50,\n",
    "   in_channels = 2*(len(exp_dict_all[SEGMENT]['inputs']['phases'])), out_channels = 1, sequence_len = 20,rnn_hid_size = 50, output_size=2, teacher_forcing=0.3,\n",
    "    lr=1e-4,\n",
    "    num_epochs=1000,\n",
    "    patience=10,\n",
    "TIME_SLICE_NAME = 'exemplarid',\n",
    "store_path = '/blue/ranka/yashaswikarnati/interruption/leakage_modelling/train_data/',\n",
    "                   processed_run_name = 'run_2022_05_to_08_ts_yash',  data_params = {'inp_agg_level':4,\n",
    "                          'oup_agg_level':20,\n",
    "                      'oup_window_use': (0,40)}, segment_name = SEGMENT\n",
    ")\n",
    "\n",
    "\n",
    "logging.info(f\"{torch.cuda.is_available()}, {torch.cuda.get_device_name(0)}\")\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device('cuda:0' if use_cuda else 'cpu')\n",
    "\n",
    "EXP_NAME = SEGMENT\n",
    "# mlflow.set_experiment(experiment_name=EXP_NAME)\n",
    "LOAD_PRETRAINED = True\n",
    "pretrained_epoch = 998"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ca3fa78b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading pretrained model segment_1425_1430_epoch_998.pth\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "\n",
    "model =  EncoderDecoderWrapper(in_channels = params.in_channels, out_channels = params.out_channels,\n",
    "                                    sequence_len = params.sequence_len,rnn_hid_size = params.rnn_hid_size, device = device, spatial_inp_size= 4 + int(params.in_channels/2),output_size=params.output_size,\n",
    "                                    teacher_forcing=params.teacher_forcing,learning_rate = params.lr)\n",
    "if(LOAD_PRETRAINED):\n",
    "    print(f\"loading pretrained model {EXP_NAME}_epoch_{pretrained_epoch}.pth\")\n",
    "    model.load_state_dict(torch.load(f'pthfiles/{EXP_NAME}_epoch_{pretrained_epoch}.pth'))\n",
    "\n",
    "trainer_obj =  Trainer(\n",
    "    model  = model,\n",
    "    device = device,\n",
    "    exp_name = EXP_NAME,\n",
    "    exp = 'segment_leak',\n",
    "    loss_fn = torch.nn.MSELoss()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04a66190",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-07 14:48:28 INFO     TRAIN SET SIZE 8992 VALIDATION SET SIZE 2247\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader = trainer_obj.get_dataloaders(params.batch_size,params.store_path,params.processed_run_name,params.data_params, params.segment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cb1e506",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_batch = next(iter(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3988af7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2e2ffc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from numpy.random import Generator, PCG64\n",
    "import uuid\n",
    "from functools import partial\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "def loop_inplace_sum(arrlist):\n",
    "    # assumes len(arrlist) > 0\n",
    "    sum = arrlist[0].copy()\n",
    "    for a in arrlist[1:]:\n",
    "        sum += a\n",
    "    return sum\n",
    "\n",
    "def return_aggregated_x(X_batch, j):\n",
    "    return X_batch.reshape(-1,j).sum(axis=1).reshape(X_batch.shape[0],-1)\n",
    "\n",
    "\n",
    "class LeakageDataset(Dataset):\n",
    "    def __init__(self, store_path,processed_run_name,segment_name,  data_params):\n",
    "        \n",
    "        self.store_path = store_path\n",
    "        self.processed_data_path = os.path.join(self.store_path, processed_run_name,segment_name)\n",
    "        self.samples_name = 'exemplar_'\n",
    "        self.all_files = os.listdir(self.processed_data_path)\n",
    "        \n",
    "        \n",
    "#         all processing constants\n",
    "        self.inp_agg_level = data_params['inp_agg_level']\n",
    "        self.oup_agg_level = data_params['oup_agg_level']\n",
    "        self.oup_window_use = data_params['oup_window_use']\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "\n",
    "        return len(os.listdir(self.processed_data_path))\n",
    "    \n",
    "    \n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return os.listdir(self.processed_data_path)\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return os.listdir(self.processed_data_path)\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        random_choice = random.choice(self.all_files)\n",
    "        data = torch.load(os.path.join(self.processed_data_path,random_choice ))        \n",
    "        x,sig,y,params, timestamp = self.return_isc_x_y(data)\n",
    "        \n",
    "        time_split_arr = [timestamp.year, timestamp.month, timestamp.day,timestamp.hour, timestamp.minute, timestamp.second ]\n",
    "        return {'x':x.transpose(1,0),'sig':sig.transpose(1,0),'y':y.reshape(-1,1) ,'params':params, 'timestamp':time_split_arr}\n",
    "    \n",
    "    \n",
    "    \n",
    "    def return_isc_x_y(self, data):\n",
    "        inp_arr = [return_aggregated_x(x.reshape(1,-1),self.inp_agg_level ) for x in data['inp'] ]\n",
    "        oup_inp =return_aggregated_x(data['oup'][self.oup_window_use[0]:self.oup_window_use[1]].reshape(1,-1),self.oup_agg_level)\n",
    "        oup_arr = return_aggregated_x(data['oup'][self.oup_window_use[1]:].reshape(1,-1),self.oup_agg_level)\n",
    "        sig_timing = [return_aggregated_x(np.array(data['sig'][k]).reshape(1,-1),self.inp_agg_level ) for k in data['sig']]\n",
    "        tod_dow= np.array([data['hour'], data['day_of_week']])\n",
    "        \n",
    "        timestamp = data['timestamp']\n",
    "        \n",
    "        x = np.concatenate(inp_arr,axis=0)\n",
    "        sig = np.concatenate(sig_timing,axis=0)\n",
    "        y  = oup_arr.reshape(-1)\n",
    "        params = np.concatenate([oup_inp.reshape(-1),tod_dow ],axis=0)\n",
    "        \n",
    "        \n",
    "        return x,sig,y,params, timestamp\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_each_batch(self, batch_size):\n",
    "        no_files = len(self.all_files)\n",
    "        \n",
    "        batch_count = 0\n",
    "        \n",
    "        while(batch_count<no_files):\n",
    "            batch_x, batch_sig, batch_y, batch_params, batch_ts, batch_time_split = [],[],[],[],[],[]\n",
    "            for jj in range(batch_size):\n",
    "                curr_data =  torch.load(os.path.join(self.processed_data_path,self.all_files[batch_count] ))\n",
    "                x,sig,y,params, timestamp = self.return_isc_x_y(curr_data)\n",
    "                ts = np.array([timestamp])\n",
    "                time_split_arr = np.array([timestamp.year, timestamp.month, timestamp.day,timestamp.hour, timestamp.minute, timestamp.second ])\n",
    "                batch_count +=1\n",
    "                batch_x.append(x[np.newaxis])\n",
    "                batch_sig.append(sig[np.newaxis])\n",
    "                batch_y.append(y[np.newaxis])\n",
    "                batch_params.append(params[np.newaxis])\n",
    "                batch_ts.append(ts[np.newaxis])\n",
    "                batch_time_split.append(time_split_arr[np.newaxis])\n",
    "                \n",
    "                \n",
    "            batch_obj = {}\n",
    "            batch_obj['x'] = torch.Tensor(np.concatenate(batch_x,axis=0))\n",
    "            batch_obj['sig'] = torch.Tensor(np.concatenate(batch_sig,axis=0))\n",
    "            batch_obj['y'] = torch.Tensor(np.concatenate(batch_y,axis=0))\n",
    "            batch_obj['params'] = torch.Tensor(np.concatenate(batch_params,axis=0))\n",
    "            batch_obj['timestamp'] = torch.Tensor(np.concatenate(batch_time_split,axis=0))\n",
    "            \n",
    "            break\n",
    "            \n",
    "        return batch_obj\n",
    "\n",
    "            \n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "            \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8049ec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = LeakageDataset(params.store_path,params.processed_run_name, params.segment_name, params.data_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6229d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_b = dataset.get_each_batch(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bd5182ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['x', 'sig', 'y', 'params', 'timestamp'])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_b.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "73f70a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 6])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_b['timestamp'].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b895ae23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "88d80611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# curr_data =  torch.load(os.path.join(dataset.processed_data_path,dataset.all_files[0] ))\n",
    "# x,sig,y,params, timestamp = dataset.return_isc_x_y(curr_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60f5c2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2, 20)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cdc1f3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
